\documentclass[12pt]{article}

\usepackage{amsmath}
\usepackage{float}
\usepackage[margin=1.2in]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{newfloat}
\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{url}


\bibliography{ref}

\DeclareCaptionType{snip}[Typename][List of snippet]
\newenvironment{snippet}{\captionsetup{type=snippet}}{}


\DeclareFloatingEnvironment{Snippet}
\title{\bfseries Sorting Algorithms}
\author{Benjamin Hough, Erick Sanchez}
\date{May 18, 2016}

\usepackage{array}
\setlength\extrarowheight{3pt} 


\lstset{
	language=[Visual]C++,
	basicstyle=\footnotesize, 
	keywordstyle=\bfseries\ttfamily\color[rgb]{0,0,1},
	identifierstyle=\ttfamily,
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\ttfamily\color[rgb]{0.627,0.126,0.941},
	showstringspaces=false,
	numberstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	numbersep=10pt,
	tabsize=2,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	breakatwhitespace=false,
	aboveskip={1.5\baselineskip},
	columns=fixed,
	extendedchars=true
	% frame=single,
	% backgroundcolor=\color{lbcolor},
}


\begin{document}
	
	\maketitle
	
	\section*{Abstract}
	
	The abstract will go here and will look awesome when it renders
	
	\tableofcontents
	
	\pagebreak
	
	\section{Introduction} %"Introduction - Give a short but informative introduction to your chosen topic, including the current “state of the field” as appropriateOutline what you will do in the balance of the paper (ie. Explain the goals of the paper). Indicate why this topic is exciting/useful."
	
	 Sorting algorithms are a fundamental type of algorithm that many other more sophisticated algorithms are built upon.
	 In the most abstract form, sorting algorithms take in a list of unordered items and output or modify the list to the desired order, whatever it may be$~\footnote{typically numeric or alphabetic order is used}$.
	 The goal of our research was to gain a better understanding of sorting algorithms in general and to learn the nuances between the different types of algorithms available.
	 We wanted to learn what a makes a sorting algorithm efficient and what are the limitations of the efficiency.
	 
	 This paper will go over definitions associated with sorting algorithms and it will cover the two broad categories of sorting algorithms, comparison and non-comparison.
	 Next it will examine sorting efficiencies and different applications of sorting algorithms.
	 It is assumed that the reader has a basic understanding of what an algorithm is and has been exposed to pseudocode in some form.
	
	\section{Background} %"Introduce any new definitions, notation, or other background information necessary for understanding the rest of the paper. (Your audience is your classmates in Math 4, not your professor.) Depending on your subject, your background and introduction sections might be combined."
	
	Since there is coding and syntax involved in this paper, only the following is expected to be understood:\\
	
	``\textbf{\underline{input}} sequence is called an instance of the sorting problem"\cite[p.~5]{intro}
	
	``An algorithm is said to be \textbf{\underline{correct}} if, for every input instance, it halts with the correct output
	We say that a correct algorithm \textbf{\underline{solves}} the given computational problem"\cite[p.~6]{intro}
	
	``The number that we wish to be sort are also known as the \textbf{\underline{keys}}"\cite[p.~16]{intro}
	
	``An \textbf{\underline{algorithm}} is a procedure or formula for solving a problem."\cite{intro}
	
	``A \textbf{\underline{compiler}} is a computer program ... that transforms source code written in a programming language ... into another computer language …, with the latter often having a binary form known as object code."\cite{intro}
	
	``The \textbf{\underline{call stack}} is a stack data structure that stores information about the active subroutines of a computer program"\cite{wiki-callStack} Whenever a function calls a new function, this adds to the stack.
	
	``A \textbf{\underline{recursion function}} (DEF) is a function which either calls itself"\cite{wiki-recursiveFunction} When a function calls itself, there needs to be a condition for the function to stop itself, end the recursion.
	This is called \textbf{\underline{end case}}.
	The end case does not contain a function call to itself thus stopping the recursion.
	
	``The \textbf{\underline{input size}} is the value, or number, to be sorted in a sorting algorithm"\cite{intro}
	
	``The \textbf{\underline{running time}} of an algorithm is the number of instructions."\cite{intro}\\
	
	Some syntax or vocabulary will be defined in-line to help understand each section of the paper.
	
	\section{Body}
	
	What you will find in this chapter are the two different classifications of sorting algorithms, comparison sorts and non-comparison sorts.
	Each sorting algorithm will have the same outcome, a sorted list.
	But, each classification differ efficiency in large values.
	
	\subsection{Efficiency and Memory}
	
	To solve the sorting problem, using one algorithm for all cases will not always be the most efficient.
	In one case, algorithm A can be the least efficient but perhaps the most efficient in another case.
	Let us look at two different sorting algorithms insertion sort and merge sort.
	Each are comparison sorts and have different efficiencies.
	
	\paragraph{Insertion Sort Vs. Merge Sort}
	
	Insertion sort takes each element in the list and places them in the correct order.
	While merge sort breaks the list to single sublists then merges each sublist in the correct order.
	Let us write insertion sort as $c_1 \cdot\,n^2$ and merge sort as $c_2 \cdot\,n\,\log_2{n}$, where $c_1$ and $c_2$ are constants dependent on how well the algorithm is implemented and how efficient the software and compiler are.
	When we compare the two, we see that insertion sort has a factor of $n$, while merge sort has a factor of $\log_2{n}$ which is far smaller for large values of $n$.
	Let us take a deeper look by running these two algorithms on different machines, machine A and B.
	Each machine is sorting an array of 10 million numbers.
	Machine A executes 10 billion instructions per second and machine B executes only 10 million per second.
	Thus, machine A is a thousand times faster than machine B.
	Insertion sort on machine A is written in an efficient manner resulting in a value of 3 for $c_1$.
	Thus, it requires $3n^2$ instructions to sort $n$ numbers.
	For machine B, the algorithm merge sort is implemented poorly causing $c_2$ is 100.
	Merge sort is written as $100 n\log_2{n}$ instructions to sort $n$ numbers.
	By using these parameters, the runtime can be calculated as followed:
	
	Machine A takes:
	\begin{equation}
	\frac{3\cdot(10^7)^2 \;instructions}{10^{10} \;instructions/\!second}=30,000\mbox{ seconds (more than 8 hours)}
	\end{equation}
	
	And machine B takes:
	\begin{equation}	
	\frac{100\cdot 10^7\log_2{10^7}\;instructions}{10^{7} \;instructions/\!second}\approx2,300\mbox{ seconds (just under 40 minutes)}
	\end{equation}
	
	Looking at this one case, having one algorithm isn't going to be the better one for all cases.
	%todo Hardware, software, the size of the list, and computer power all add to how efficient a sorting algorithm will be.
	Why? That's when we need to side 
	
	An algorithm's runtime can vary greatly from nanoseconds, to minutes, or even to years.
	%todo Measuring resources used for an algorithm to execute, the Big O notation would give you the answer.
	But before we understand Big O notation, we’ll look into analyzing insertion sort.
	
	
	\paragraph{Analysis of Insertion Sort}
	
	The notation we use to describe the running time of an algorithm is defined in terms of a function whose domains is the set of natural numbers ($\mathbb{N}$ = \{ 1, 2 , 3 .. \})~\cite[p.~43]{intro}.
	For insertion sort, the worst-case is $n^2$, this occurs when the list is in reverse order.
	The larger a list is, the longer it'll take.
	Thus, we need to define a few things to characterize insertion sort, running time and size of input.
	For a sorting algorithm, the input size is the number of elements to be sorted in the list.
	This does not depend on how unsorted the arrangement of number, just the size of the list.
	The running time is going to be the number of steps, or instructions.
	Now, each line of code can't always take the same amount of $\textit{time}$ to execute.
	An assignment will take a different amount of $\textit{time}$ than an $\textit{if-else}$ statement.
	
	To better understand, we will look at an example of a simple loop.
	Each line of code will take $c_i$ long to execute; $i$ is denoted for each line of code.
	\input{sampleLoop}
	Thus, by adding the \textbf{times} column together, the $\textit{time}$ it takes to execute this loop is given by the equation below.
	\begin{equation}
	T(n) = c_1n + c_2n + c_3\sum_{r=1}^{5} + c_4\sum_{r=1}^{5}
	\label{eqa:simpleLoopRunningTime}
	\end{equation}
	Equation~\ref{eqa:simpleLoopRunningTime} is the characteristic formula for the algorithm shown in table~\ref{tab:simpleCode}.
	For this function it has a factor of $n$ that can be factored to T(n) = ($c_1$ + $c_2$)n + ($c_3$ + $c_4$)$\sum_{r=1}^{5}$, or T(n) = an + b.
	
	\paragraph{Describing Insertion Sort}
	Coming back to insertion sort, before we dive into the same process done in table~\ref{tab:simpleCode} to get equation~\ref{eqa:simpleLoopRunningTime}, let us find out how insertion sort behaves.
	Insertion sort will iterate from the beginning to end of the list.
	While in each iteration, a $key$ is assigned and another iteration steps backwards from the index of the first iteration shown in table~\ref{tab:insertionCode}.
	While the second iteration steps backwards, each element is shifted towards the end of the list lines 6 and 7.
	This inner iteration continues until the value of the first iteration is larger than the value of the second iteration or the second iteration steps beyond.
	The last step of the first iteration is to $\textit{insert}$ the $key$ at the index of the second iteration.
	Thus, every element will be inserted in the correct order once the first iteration completes across the size of the list being sorted.
	Since insertion sort contains a loop within a loop, the running time can be a second degree thus taking much longer.
	
	Now, creating a running equation, we have to assign each line of code with $c_i$.
	The table shown below is the running time with the pseudocode for insertion sort.
	
	\input{insertionSort-runningtime.tex}
	
	%todo wrost case first then best case% Now, the characteristic for the best case of insertion sort is
	If the worst case was sorted using insertion sort, the characteristic would be $an^2$ + $bn$ + $c$, where a, b, and c were constants as shown below
	\begin{equation}
	T(n) = (\frac{c_5}{2} + \frac{c_6}{2} + \frac{c_7}{2})n^2 + (c_1 + c_2 + c_4 + \frac{c_5}{2} - \frac{c_6}{2} - \frac{c_7}{2} + c_8)n - (c_2 + c_4 + c_5 + c_8)
	\label{eqa:insertionSortWorstRunningTime}
	\end{equation}
	
	For the best case to happen the list would have already been in the correct order.
	Line 5 in table~\ref{tab:insertionCode} would never enter the while-loop.
	Thus, the summation of $t_j$ is equal to 1.
	And we can write the characteristic for the best case as equation~\ref{eqa:insertionSortBestRunningTime}.
	
	\begin{equation}
	T(n) = (c_1 + c_2 + c_4 + c_5 + c_8)n - (c_2 + c_4 + c_5 + c_8)
	\label{eqa:insertionSortBestRunningTime}
	\end{equation}
	Equation~\ref{eqa:insertionSortBestRunningTime}.
	which can be expressed as an + b for constants a and b that depend on the statement costs $c_i$ as shown in table~\ref{tab:insertionCode}.
	
	
	
	Looking at both equation~\ref{eqa:insertionSortBestRunningTime} and equation~\ref{eqa:insertionSortWorstRunningTime}, one is linear but the other is exponential.
	For some cases, insertion sort will be the most efficient but not for every case.
	
	\subsection{Comparison Sorts}
	\label{CompSort}
	Comparison sorts are any sorting algorithms that use logical comparisons to determine element order, typically $>$, $<$, $\ge$, or $\le$ is used.
	
	\paragraph{Insertion Sort}
	
	
	\paragraph{title}
	
	%todo What are they, How do they work, When and when not to use them
	
	\input{comparisonSortTable} %rad table file
	
	\subsection{Non-Comparison Sorts}
	
	Non-comparison sorts, like the name implies do not rely on the comparison operators to sort.
	This gives non-comparison sorts a huge speed boost but of course there is a trade off to the increased speed.
	Unlike the comparison sorts described in \ref{CompSort} which can sort any data that has a defined numerical value, non-comparison sorts rely on certain assumptions about what is being sorted in order to function.
	This sets certain restrictions on the type of data that non-comparison sorts can manage efficiently.
	The types of restriction vary depending on the sort used, but typical restrictions include the range of values, the distribution of values\footnote{Bucket sort is an example of this which requires an even distribution of values to perform well}, and the number of digits or bits per key.
	Common non-comparisons sorts include: radix sort, burstsort, postman sort, bucket sort, and counting sort.
	Table~\ref{tab:nonComparisionEfficency} lists the average and worst cases for each sorting algorithm.
	We will look at an examples of counting sort in order to gain an understanding of what makes non-comparison sorts different.

	\input{non-comparisonSortTable} %rad table file
	
	\begin{center}
		\captionof{Snippet}[Counting sort]{Pseudocode for counting sort\cite[p.~195]{intro}}
		\lstinputlisting[xleftmargin=2in]{countingsort.cpp}
		\label{snip:countsort}
	\end{center}
	
	Counting sort has 3 for loops which make up the 3 main steps. 
	Notice that there are no comparison operators.
	The first loop counts the occurrences of each number, the second adds up the offset for each element caused by the numbers that precede it, and the third moves the numbers into sorted order.
	\begin{figure}[H]
		
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=1.2in]{_0000_count0.png} 
			\caption{Before sorting}
			\label{fig:count1}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=1.2in]{_0001_count1.png}
			\caption{After 1st loop}
			\label{fig:count2}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=1.2in]{_0002_count2.png}
			\caption{After 2nd loop}
			\label{fig:count3}
		\end{subfigure}
		\begin{subfigure}{0.24\textwidth}
			\includegraphics[width=1.2in]{_0003_count3.png}
			\caption{after 3rd loop}
			\label{fig:count4}
		\end{subfigure}

		\caption{Counting sort progression}
		\label{fig:countmerge}
	\end{figure}
	
	\subsection{Exploration of a New Sorting Algorithm}
	
	To fulfill the requirement of demonstrating original thought we looked at creating our own sorting algorithm.
	It proved difficult to come with a new sorting idea with so many existing sorting algorithm already on our mind.
	So instead we took a well known sorting algorithm, merge sort, and put a twist on it.
	The standard implementation of mergesort uses a recursive flow; each iteration calls the sort on itself and splits the list in two until it is completely divided.
	%todo It then pieces it back together as it sorts
	A visual representation of this is shown in Figure~\ref{fig:stdmerge}.
	
	
\begin{figure}[H]
	
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=2.5in]{Merge_sort_algorithm_diagram.png} 
		\caption{Standard Mergesort}
		\label{fig:stdmerge}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\includegraphics[width=2.5in]{New_merge_sort_algorithm_diagram.png}
		\caption{Our Mergesort}
		\label{fig:newmerge}
	\end{subfigure}
	
	\caption{Algorithm flow of the standard merge sort and our own implementation}
	\label{fig:bothmerge}
\end{figure}
	
	Now onto our variant.
	We decided to forgo the recursion and keep everything to one function call.
	The reasoning for this is, recursive functions have an overhead associated with them caused by the increased call stack \footnote{local variables must be reinitialized at each recursion level and the entrance of each function call must be stored so it can be returned to when the function exits}
	By skipping the recursion step it allows the algorithm to first start comparing pairs of elements.
	Visually it is represented by Figure~\ref{fig:newmerge}.
	Do not let the Figure~\ref{fig:newmerge} fool you however.
	Just because it looks like half the number of steps does not mean it is twice as fast.
	Before we go into the efficiency here is algorithm in its entirety is shown below in Snippet~\ref{snip:newmerge}.
	
	\begin{center}
		\captionof{Snippet}[New Mergesort]{Our own merge sort implementation that doesn't use recursion. Written in C++}
		\lstinputlisting{mergeSort2.cpp}
		\label{snip:newmerge}
	\end{center}

	Just knowing that an algorithm works is not enough\footnote{We do not prove that the new merge sort works, you must take our word that it does.}; an analysis of the runtime is also needed to ensure the algorithm performs as needed.
	The new merge sort algorithm was compared to a standard implementation of merge sort, introsort, quicksort, and insertion sort.
	To perform the tests, each algorithm was given the same randomly generated list starting with a size of 3 and doubling each time up to 12,582,912 elements.
	To our surprise the new merge sort outperformed many of the other algorithms.
	As shown in Figure~\ref{fig:algeff}, for lists with over 100 elements and up to the test limit the new merge sort performed 13X faster than the built in introsort and 3.6X faster than the standard merge sort.
	The efficiency of insertion sort on small list sizes can be seen in the graph as well.
	Quicksort is consistently faster than the new merge sort, with quicksort averaging 80\% of the runtime over the entire sample range.
	However, it can be seen that at around 2 million elements, the new merge sort pulled ahead.
	Each algorithm in Figure~\ref{fig:algeff} with the exception of insertion sort has an average runtime of $n \cdot log (n)$, yet it is clear from the plot that this approximation is not always precise enough when looking for the absolute fastest algorithm.

	\begin{figure}[H]
		\centering
		\includegraphics[width=6in]{figure_3.png}
		\caption{Algorithm Efficiency}
		\label{fig:algeff}
	\end{figure}
	
\section{Conclusion}

\pagebreak

\begin{thebibliography}{9}
	\bibitem{intro}
	Introduction to Algorithms, Third Edition,
	% todo: update author \emph{\LaTeX: a document preparation system},
	Addison Wesley, Massachusetts,
	2nd edition,
	1994.
	
	\bibitem{wiki-callStack}
	Wikipedia–~\url{https://en.wikipedia.org/wiki/Call_stack}
	
	\bibitem{wiki-recursiveFunction}
	Wikipedia–~\url{https://en.wikipedia.org/wiki/Recursive_function}
	
\end{thebibliography}

\end{document}

